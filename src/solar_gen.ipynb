{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "solar_gen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "inDNCSssI9c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492ffbe0-5e95-41e2-b696-dfba258b694f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G12GfaP3dZj"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import random\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "np.random.sample(1337)\r\n",
        "\r\n",
        "data_path = '/content/drive/MyDrive/SolarGen/'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr2w7mXvRGKf"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmcCM4y0RGKf"
      },
      "source": [
        "df_train = pd.read_csv(data_path + './data/train/train.csv')\r\n",
        "submission = pd.read_csv(data_path + './data/sample_submission.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8hOU5CLRGKg",
        "outputId": "a3443f6f-8c42-4417-911b-8fac527c2c3f"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import (\r\n",
        "    MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, \r\n",
        "    MaxAbsScaler, PowerTransformer\r\n",
        ")\r\n",
        "\r\n",
        "class ScalingFactor:\r\n",
        "    def __init__(self, scaler, features):\r\n",
        "        self.scaler = scaler\r\n",
        "        self.features = features\r\n",
        "\r\n",
        "def scaling(data, scaler, features, is_train=True):\r\n",
        "\r\n",
        "    temp = data.copy()\r\n",
        "\r\n",
        "    target_cols = features\r\n",
        "    temp = temp[target_cols]\r\n",
        "    if is_train: \r\n",
        "        scaler.fit(temp)\r\n",
        "    scaled = scaler.transform(temp)\r\n",
        "\r\n",
        "    data[target_cols] = scaled\r\n",
        "\r\n",
        "    return data\r\n",
        "\r\n",
        "def slicing_data(data, window_size):\r\n",
        "\r\n",
        "    slices = []\r\n",
        "\r\n",
        "    for i in range(len(data) - window_size):\r\n",
        "        slices.append(np.array(data[i:i+window_size]))\r\n",
        "\r\n",
        "    return np.array(slices)\r\n",
        "\r\n",
        "def preprocess_data(data, scale_factors, window_size, is_train=True):\r\n",
        "    \r\n",
        "    temp = data.copy()\r\n",
        "\r\n",
        "    for factor in scale_factors:\r\n",
        "        temp = scaling(temp, factor.scaler, factor.features, is_train)\r\n",
        "\r\n",
        "    temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\r\n",
        "    temp_np = temp.to_numpy()\r\n",
        "\r\n",
        "    if is_train==True:          \r\n",
        "    \r\n",
        "        temp = slicing_data(temp_np, window_size)\r\n",
        "        \r\n",
        "        return temp[:-96]\r\n",
        "\r\n",
        "    elif is_train==False:\r\n",
        "\r\n",
        "        temp = slicing_data(temp_np, window_size)\r\n",
        "                              \r\n",
        "        return temp[-1]\r\n",
        "\r\n",
        "scale_factors = []\r\n",
        "scale_factors.append(ScalingFactor(PowerTransformer(), ['DHI', 'DNI']))\r\n",
        "# scale_factors.append(ScalingFactor(QuantileTransformer(output_distribution='normal'), ['DNI']))\r\n",
        "scale_factors.append(ScalingFactor(StandardScaler(), ['WS', 'RH', 'T']))\r\n",
        "scale_factors.append(ScalingFactor(MinMaxScaler(), ['DHI', 'DNI', 'WS', 'RH', 'T']))\r\n",
        "\r\n",
        "window_size = 48\r\n",
        "train = preprocess_data(df_train, scale_factors, window_size)\r\n",
        "train_label = slicing_data(df_train.iloc[window_size:]['TARGET'], 48)\r\n",
        "\r\n",
        "# train[0].iloc[:, 1:].plot(subplots=True, layout=(1,6), figsize=(30,4))\r\n",
        "# train[0].iloc[:, 1:].hist(bins = 100, layout=(1,6), figsize=(30,4))\r\n",
        "train.shape, train_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52416, 48, 7), (52416, 96))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "hMopCEGLLLzu",
        "outputId": "349461d8-f6e2-4882-f1f8-804e248e3fbb"
      },
      "source": [
        "nth_data = 0\r\n",
        "fig, ax = plt.subplots(figsize=(16,3))\r\n",
        "ax.scatter([i+1 for i in range(48)], train[nth_data][:,1], color='k')\r\n",
        "ax.scatter([i+49 for i in range(48)], train_label[nth_data], color='g')\r\n",
        "ax.scatter([i+97 for i in range(48)], train_label[nth_data+48], color='y')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f88bda83f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maJnPTaoRGKg",
        "outputId": "b45a6dbb-6e63-47a9-bd06-79d82d28101b"
      },
      "source": [
        "test = []\r\n",
        "\r\n",
        "for i in range(81):\r\n",
        "    file_path = data_path + './data/test/' + str(i) + '.csv'\r\n",
        "    temp = pd.read_csv(file_path)\r\n",
        "    temp = preprocess_data(temp, scale_factors, window_size, is_train=False)\r\n",
        "    test.append(temp)\r\n",
        "\r\n",
        "# X_test[0].iloc[:, 1:].plot(subplots=True, layout=(1,6), figsize=(30,4))\r\n",
        "# X_test[0].iloc[:, 1:].hist(bins = 100, layout=(1,6), figsize=(30,4))\r\n",
        "\r\n",
        "X_test = np.array(test)\r\n",
        "X_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81, 48, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxPyhppkRGKg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train1, X_valid1, Y_train1, Y_valid1 = train_test_split(train, train_label[:-48], test_size=0.3, random_state=0, shuffle=False)\r\n",
        "X_train2, X_valid2, Y_train2, Y_valid2 = train_test_split(train, train_label[48:], test_size=0.3, random_state=0, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "DpkOF72yNegB",
        "outputId": "6347201a-f2b1-41eb-c7ac-7c7db93a68c8"
      },
      "source": [
        "nth_data = 0\r\n",
        "fig, ax = plt.subplots(figsize=(16,3))\r\n",
        "ax.scatter([i+1 for i in range(48)], X_train1[nth_data][:,1], color='k')\r\n",
        "ax.scatter([i+49 for i in range(48)], Y_train1[nth_data], color='g')\r\n",
        "ax.scatter([i+97 for i in range(48)], Y_train2[nth_data], color='y')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f88ae22aa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atOKtUkHRGKh"
      },
      "source": [
        "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_8Jzf9lnlpe",
        "outputId": "3e230da7-9184-49fe-dc32-6d22e73128cc"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense\r\n",
        "\r\n",
        "rnn_layer = 48\r\n",
        "hidden_layer = 96\r\n",
        "\r\n",
        "models1 = []\r\n",
        "models2 = []\r\n",
        "\r\n",
        "def ploss(q):\r\n",
        "\r\n",
        "    def pinball_loss(y_true, y_pred):\r\n",
        "        error = y_true - y_pred\r\n",
        "        return K.mean(K.maximum(q*error, (q-1)*error), axis=-1)\r\n",
        "\r\n",
        "    return pinball_loss\r\n",
        "\r\n",
        "for q in quantiles:\r\n",
        "    rnn = tf.keras.Sequential()\r\n",
        "    rnn.add(LSTM(rnn_layer, input_shape=X_train1[0].shape))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(Y_train1[0].shape[0]))\r\n",
        "    rnn.compile(loss=ploss(q=q), optimizer='adam')\r\n",
        "    models1.append(rnn)\r\n",
        "\r\n",
        "for q in quantiles:\r\n",
        "    rnn = tf.keras.Sequential()\r\n",
        "    rnn.add(LSTM(rnn_layer, input_shape=X_train2[0].shape))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(hidden_layer, activation='relu'))\r\n",
        "    rnn.add(Dense(Y_train2[0].shape[0]))\r\n",
        "    rnn.compile(loss=ploss(q=q), optimizer='adam')\r\n",
        "    models2.append(rnn)\r\n",
        "\r\n",
        "models1[0].summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 48)                10752     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 96)                9312      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 96)                9312      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 96)                9312      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 96)                9312      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 48)                4656      \n",
            "=================================================================\n",
            "Total params: 57,360\n",
            "Trainable params: 57,360\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RyyQnEZtMbq",
        "outputId": "4437de5e-0c3b-438a-c590-13b2dce35b04"
      },
      "source": [
        "batch_size = 128\r\n",
        "epochs = 200\r\n",
        "\r\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min')\r\n",
        "\r\n",
        "for index, model in enumerate(models1):\r\n",
        "    print(\"[Q = %.1f]\"%((index+1)*0.1))\r\n",
        "    best_model = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/SolarGen/model/m1_best_%d.h5'%((index+1)), \r\n",
        "                                                monitor='val_loss', verbose=1, save_best_only=True)\r\n",
        "    model.fit(X_train1, Y_train1, validation_data=(X_valid1, Y_valid1), epochs=epochs, \r\n",
        "              batch_size=batch_size, callbacks=[early_stopping, best_model])\r\n",
        "\r\n",
        "for index, model in enumerate(models2):\r\n",
        "    print(\"[Q = %.1f]\"%((index+1)*0.1))\r\n",
        "    best_model = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/SolarGen/model/m2_best_%d.h5'%((index+1)), \r\n",
        "                                                monitor='val_loss', verbose=1, save_best_only=True)\r\n",
        "    model.fit(X_train2, Y_train2, validation_data=(X_valid2, Y_valid2), epochs=epochs, \r\n",
        "              batch_size=batch_size, callbacks=[early_stopping, best_model])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Q = 0.1]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 5s 7ms/step - loss: 1.7303 - val_loss: 1.5770\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.57702, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4031 - val_loss: 1.4733\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.57702 to 1.47333, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3491 - val_loss: 1.4499\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.47333 to 1.44995, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3244 - val_loss: 1.4553\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.44995\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3158 - val_loss: 1.4504\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.44995\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3113 - val_loss: 1.4432\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.44995 to 1.44320, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3039 - val_loss: 1.4494\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.44320\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2861 - val_loss: 1.4367\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.44320 to 1.43671, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2866 - val_loss: 1.4381\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.43671\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2870 - val_loss: 1.4390\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.43671\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2763 - val_loss: 1.4359\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.43671 to 1.43593, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2716 - val_loss: 1.4466\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.43593\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2600 - val_loss: 1.4446\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.43593\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2540 - val_loss: 1.4320\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.43593 to 1.43197, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_1.h5\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2499 - val_loss: 1.4346\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.43197\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2453 - val_loss: 1.4370\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.43197\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2309 - val_loss: 1.5037\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.43197\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2314 - val_loss: 1.4423\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.43197\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2254 - val_loss: 1.4621\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.43197\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2223 - val_loss: 1.4688\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.43197\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2153 - val_loss: 1.4591\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.43197\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.1940 - val_loss: 1.4891\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.43197\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.1984 - val_loss: 1.4906\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.43197\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.1870 - val_loss: 1.4869\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.43197\n",
            "[Q = 0.2]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 3.2648 - val_loss: 2.5894\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.58936, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2531 - val_loss: 2.4104\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.58936 to 2.41041, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1794 - val_loss: 2.3632\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.41041 to 2.36315, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1400 - val_loss: 2.3542\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.36315 to 2.35424, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1224 - val_loss: 2.3400\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.35424 to 2.33999, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0857 - val_loss: 2.3652\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.33999\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0710 - val_loss: 2.3064\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.33999 to 2.30635, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_2.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0612 - val_loss: 2.3730\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.30635\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0385 - val_loss: 2.3229\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.30635\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0071 - val_loss: 2.3164\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.30635\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9967 - val_loss: 2.3335\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.30635\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9743 - val_loss: 2.3754\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.30635\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9712 - val_loss: 2.3484\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.30635\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9639 - val_loss: 2.3499\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.30635\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9450 - val_loss: 2.3300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.30635\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9381 - val_loss: 2.3626\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.30635\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9169 - val_loss: 2.3753\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.30635\n",
            "[Q = 0.3]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 4.4869 - val_loss: 3.1018\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.10182, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7303 - val_loss: 2.9161\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.10182 to 2.91613, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6478 - val_loss: 2.8280\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.91613 to 2.82797, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5741 - val_loss: 2.8385\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 2.82797\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5406 - val_loss: 2.8207\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.82797 to 2.82065, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5006 - val_loss: 2.7995\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.82065 to 2.79955, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4905 - val_loss: 2.7858\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.79955 to 2.78581, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4479 - val_loss: 2.7764\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.78581 to 2.77642, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4362 - val_loss: 2.7839\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.77642\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4520 - val_loss: 2.7732\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.77642 to 2.77317, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4111 - val_loss: 2.7335\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.77317 to 2.73352, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_3.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3925 - val_loss: 2.7585\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.73352\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3896 - val_loss: 2.7666\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.73352\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3959 - val_loss: 2.7574\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.73352\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3498 - val_loss: 2.7594\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.73352\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3281 - val_loss: 2.7803\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.73352\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3455 - val_loss: 2.7461\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.73352\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3273 - val_loss: 2.7880\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.73352\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3152 - val_loss: 2.7675\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.73352\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2893 - val_loss: 2.8169\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.73352\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2810 - val_loss: 2.7938\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.73352\n",
            "[Q = 0.4]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 5.5231 - val_loss: 3.2728\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.27278, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9773 - val_loss: 3.1754\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.27278 to 3.17538, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8367 - val_loss: 3.0537\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.17538 to 3.05366, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7527 - val_loss: 3.0120\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.05366 to 3.01199, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7010 - val_loss: 2.9600\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.01199 to 2.95996, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6767 - val_loss: 2.9437\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.95996 to 2.94366, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6323 - val_loss: 2.9347\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.94366 to 2.93469, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6090 - val_loss: 2.9483\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.93469\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6109 - val_loss: 2.9389\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.93469\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5722 - val_loss: 2.9252\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.93469 to 2.92522, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_4.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5598 - val_loss: 3.0049\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.92522\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5323 - val_loss: 2.9606\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.92522\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5210 - val_loss: 2.9371\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.92522\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5180 - val_loss: 2.9376\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.92522\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4805 - val_loss: 3.0196\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.92522\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4717 - val_loss: 2.9836\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.92522\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4528 - val_loss: 2.9874\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.92522\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4680 - val_loss: 2.9929\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.92522\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4376 - val_loss: 2.9950\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.92522\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4181 - val_loss: 3.0218\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.92522\n",
            "[Q = 0.5]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 6.4278 - val_loss: 3.2684\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.26844, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9540 - val_loss: 3.0867\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.26844 to 3.08674, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8009 - val_loss: 2.9487\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.08674 to 2.94871, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7102 - val_loss: 2.9340\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.94871 to 2.93398, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6444 - val_loss: 2.8903\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.93398 to 2.89034, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6143 - val_loss: 2.9303\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.89034\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5738 - val_loss: 2.8446\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.89034 to 2.84464, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_5.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5265 - val_loss: 2.8615\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.84464\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5091 - val_loss: 2.9214\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.84464\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4816 - val_loss: 2.8659\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.84464\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4642 - val_loss: 2.9267\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.84464\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4460 - val_loss: 2.8949\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.84464\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4464 - val_loss: 2.8698\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.84464\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4059 - val_loss: 2.8713\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.84464\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3904 - val_loss: 2.9063\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.84464\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3828 - val_loss: 2.8608\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.84464\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3549 - val_loss: 2.9187\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.84464\n",
            "[Q = 0.6]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 7.6205 - val_loss: 3.0131\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.01307, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7688 - val_loss: 2.9281\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.01307 to 2.92806, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6235 - val_loss: 2.7036\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.92806 to 2.70357, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4829 - val_loss: 2.6146\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.70357 to 2.61459, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4123 - val_loss: 2.5798\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.61459 to 2.57981, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3478 - val_loss: 2.5984\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.57981\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3398 - val_loss: 2.5703\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.57981 to 2.57028, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3363 - val_loss: 2.5731\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.57028\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2890 - val_loss: 2.5383\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.57028 to 2.53830, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2637 - val_loss: 2.5398\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.53830\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2565 - val_loss: 2.6131\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.53830\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2616 - val_loss: 2.5094\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.53830 to 2.50938, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_6.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2179 - val_loss: 2.5471\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.50938\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2052 - val_loss: 2.5598\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.50938\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1865 - val_loss: 2.5537\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.50938\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1735 - val_loss: 2.5285\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.50938\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1565 - val_loss: 2.5486\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.50938\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1454 - val_loss: 2.5715\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.50938\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1394 - val_loss: 2.5504\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.50938\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1421 - val_loss: 2.5459\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.50938\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1168 - val_loss: 2.5196\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.50938\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1137 - val_loss: 2.5155\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.50938\n",
            "[Q = 0.7]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 4s 7ms/step - loss: 7.9274 - val_loss: 2.6856\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.68561, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4559 - val_loss: 2.4646\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.68561 to 2.46458, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2820 - val_loss: 2.3432\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.46458 to 2.34322, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1746 - val_loss: 2.2489\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.34322 to 2.24888, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0569 - val_loss: 2.2157\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.24888 to 2.21571, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0216 - val_loss: 2.1840\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.21571 to 2.18403, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9929 - val_loss: 2.1579\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.18403 to 2.15792, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9589 - val_loss: 2.0965\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.15792 to 2.09655, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9353 - val_loss: 2.0982\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.09655\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9230 - val_loss: 2.1302\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.09655\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9220 - val_loss: 2.1089\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.09655\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8804 - val_loss: 2.0761\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.09655 to 2.07613, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8970 - val_loss: 2.0890\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.07613\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8679 - val_loss: 2.0689\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.07613 to 2.06888, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8621 - val_loss: 2.0810\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.06888\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8725 - val_loss: 2.0566\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.06888 to 2.05658, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8589 - val_loss: 2.0656\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.05658\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8524 - val_loss: 2.1043\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.05658\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8357 - val_loss: 2.1108\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.05658\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8265 - val_loss: 2.0704\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.05658\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8295 - val_loss: 2.0536\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.05658 to 2.05359, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8173 - val_loss: 2.0647\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.05359\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8085 - val_loss: 2.0504\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.05359 to 2.05037, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8046 - val_loss: 2.0796\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.05037\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7931 - val_loss: 2.0743\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.05037\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7998 - val_loss: 2.0449\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.05037 to 2.04494, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7821 - val_loss: 2.0689\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.04494\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7800 - val_loss: 2.0803\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.04494\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7877 - val_loss: 2.0945\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 2.04494\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7728 - val_loss: 2.1058\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.04494\n",
            "Epoch 31/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7770 - val_loss: 2.0406\n",
            "\n",
            "Epoch 00031: val_loss improved from 2.04494 to 2.04063, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_7.h5\n",
            "Epoch 32/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7617 - val_loss: 2.0661\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 2.04063\n",
            "Epoch 33/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7479 - val_loss: 2.1163\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 2.04063\n",
            "Epoch 34/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7594 - val_loss: 2.0620\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 2.04063\n",
            "Epoch 35/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7566 - val_loss: 2.0927\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.04063\n",
            "Epoch 36/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7514 - val_loss: 2.0581\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 2.04063\n",
            "Epoch 37/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7340 - val_loss: 2.0415\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.04063\n",
            "Epoch 38/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7339 - val_loss: 2.0781\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 2.04063\n",
            "Epoch 39/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7170 - val_loss: 2.0813\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 2.04063\n",
            "Epoch 40/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7078 - val_loss: 2.1226\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 2.04063\n",
            "Epoch 41/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7356 - val_loss: 2.1146\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 2.04063\n",
            "[Q = 0.8]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 9.2980 - val_loss: 2.2784\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.27840, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9475 - val_loss: 1.8797\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.27840 to 1.87969, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7884 - val_loss: 1.8706\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.87969 to 1.87061, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7314 - val_loss: 1.7694\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.87061 to 1.76936, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.6448 - val_loss: 1.7049\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.76936 to 1.70492, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5834 - val_loss: 1.6470\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.70492 to 1.64700, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5207 - val_loss: 1.6776\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.64700\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4947 - val_loss: 1.5605\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.64700 to 1.56045, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4569 - val_loss: 1.5602\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.56045 to 1.56016, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4328 - val_loss: 1.5519\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.56016 to 1.55186, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 1.4153 - val_loss: 1.5505\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.55186 to 1.55049, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4033 - val_loss: 1.5637\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.55049\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4065 - val_loss: 1.5038\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.55049 to 1.50377, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3999 - val_loss: 1.5362\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.50377\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3798 - val_loss: 1.4902\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.50377 to 1.49020, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3703 - val_loss: 1.6394\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.49020\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3641 - val_loss: 1.5082\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.49020\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3586 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.49020\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3668 - val_loss: 1.4743\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.49020 to 1.47431, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_8.h5\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3441 - val_loss: 1.5118\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.47431\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3532 - val_loss: 1.5039\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.47431\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3336 - val_loss: 1.5275\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.47431\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3259 - val_loss: 1.5091\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.47431\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3364 - val_loss: 1.5214\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.47431\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3195 - val_loss: 1.4796\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.47431\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3123 - val_loss: 1.5049\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.47431\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3081 - val_loss: 1.5245\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.47431\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.2932 - val_loss: 1.5041\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.47431\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3024 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.47431\n",
            "[Q = 0.9]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 8.7944 - val_loss: 3.0837\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.08365, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7113 - val_loss: 2.6194\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.08365 to 2.61940, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9866 - val_loss: 1.1214\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.61940 to 1.12141, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.0513 - val_loss: 1.0314\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.12141 to 1.03142, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9969 - val_loss: 1.0032\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.03142 to 1.00321, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9612 - val_loss: 0.9646\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.00321 to 0.96464, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9288 - val_loss: 0.9484\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.96464 to 0.94836, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8905 - val_loss: 0.9251\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.94836 to 0.92506, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8682 - val_loss: 0.9145\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.92506 to 0.91452, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8489 - val_loss: 0.8873\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.91452 to 0.88728, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8350 - val_loss: 0.8836\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.88728 to 0.88364, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8215 - val_loss: 0.9059\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.88364\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8158 - val_loss: 0.8774\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.88364 to 0.87736, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7984 - val_loss: 0.8445\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.87736 to 0.84446, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7874 - val_loss: 0.8312\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.84446 to 0.83119, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7869 - val_loss: 0.8422\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.83119\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7798 - val_loss: 0.8343\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.83119\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7755 - val_loss: 0.8573\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.83119\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7784 - val_loss: 0.8259\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.83119 to 0.82589, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7715 - val_loss: 0.8248\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.82589 to 0.82480, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7653 - val_loss: 0.8418\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.82480\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7653 - val_loss: 0.8112\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.82480 to 0.81118, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7691 - val_loss: 0.8149\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.81118\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7518 - val_loss: 0.8235\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.81118\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7538 - val_loss: 0.8861\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.81118\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7515 - val_loss: 0.8041\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.81118 to 0.80410, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7436 - val_loss: 0.7907\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.80410 to 0.79068, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7335 - val_loss: 0.8181\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.79068\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7405 - val_loss: 0.8077\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.79068\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7419 - val_loss: 0.8213\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.79068\n",
            "Epoch 31/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7344 - val_loss: 0.8048\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.79068\n",
            "Epoch 32/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7315 - val_loss: 0.8186\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.79068\n",
            "Epoch 33/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7368 - val_loss: 0.8155\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.79068\n",
            "Epoch 34/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7355 - val_loss: 0.8181\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.79068\n",
            "Epoch 35/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7268 - val_loss: 0.7865\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.79068 to 0.78653, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 36/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7227 - val_loss: 0.8064\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.78653\n",
            "Epoch 37/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7132 - val_loss: 0.8154\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.78653\n",
            "Epoch 38/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7251 - val_loss: 0.7934\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.78653\n",
            "Epoch 39/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7208 - val_loss: 0.8046\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.78653\n",
            "Epoch 40/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7192 - val_loss: 0.7932\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.78653\n",
            "Epoch 41/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7206 - val_loss: 0.7882\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.78653\n",
            "Epoch 42/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7158 - val_loss: 0.7964\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.78653\n",
            "Epoch 43/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7161 - val_loss: 0.8243\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.78653\n",
            "Epoch 44/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7237 - val_loss: 0.7739\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.78653 to 0.77390, saving model to /content/drive/MyDrive/SolarGen/model/m1_best_9.h5\n",
            "Epoch 45/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7130 - val_loss: 0.7871\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.77390\n",
            "Epoch 46/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7148 - val_loss: 0.7844\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.77390\n",
            "Epoch 47/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7211 - val_loss: 0.7925\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.77390\n",
            "Epoch 48/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7142 - val_loss: 0.7851\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.77390\n",
            "Epoch 49/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7114 - val_loss: 0.7811\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.77390\n",
            "Epoch 50/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7047 - val_loss: 0.7789\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.77390\n",
            "Epoch 51/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7038 - val_loss: 0.7900\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.77390\n",
            "Epoch 52/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7072 - val_loss: 0.7910\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.77390\n",
            "Epoch 53/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7104 - val_loss: 0.7932\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.77390\n",
            "Epoch 54/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.6992 - val_loss: 0.7798\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.77390\n",
            "[Q = 0.1]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 4s 7ms/step - loss: 1.7395 - val_loss: 1.6963\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.69630, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4938 - val_loss: 1.5466\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.69630 to 1.54663, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4226 - val_loss: 1.5354\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.54663 to 1.53543, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4044 - val_loss: 1.5327\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.53543 to 1.53273, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4020 - val_loss: 1.5268\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.53273 to 1.52676, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3972 - val_loss: 1.5258\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.52676 to 1.52581, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3913 - val_loss: 1.5246\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.52581 to 1.52456, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_1.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3828 - val_loss: 1.5278\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.52456\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3863 - val_loss: 1.5334\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.52456\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3723 - val_loss: 1.5262\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.52456\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3671 - val_loss: 1.5320\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.52456\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3572 - val_loss: 1.5516\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.52456\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3538 - val_loss: 1.5349\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.52456\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3525 - val_loss: 1.5497\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.52456\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3287 - val_loss: 1.5430\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.52456\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3223 - val_loss: 1.6071\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.52456\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3288 - val_loss: 1.5529\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.52456\n",
            "[Q = 0.2]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 7ms/step - loss: 3.2658 - val_loss: 2.6436\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.64360, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_2.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4250 - val_loss: 2.6132\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.64360 to 2.61315, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_2.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3889 - val_loss: 2.5600\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.61315 to 2.56004, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_2.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3500 - val_loss: 2.5429\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.56004 to 2.54292, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_2.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3311 - val_loss: 2.5308\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.54292 to 2.53081, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_2.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3243 - val_loss: 2.5347\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.53081\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3042 - val_loss: 2.5367\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 2.53081\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2896 - val_loss: 2.5660\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.53081\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2522 - val_loss: 2.5381\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.53081\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2517 - val_loss: 2.5563\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.53081\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2300 - val_loss: 2.5541\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.53081\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2077 - val_loss: 2.5895\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.53081\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1965 - val_loss: 2.5448\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.53081\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1862 - val_loss: 2.5559\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.53081\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1651 - val_loss: 2.5807\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.53081\n",
            "[Q = 0.3]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 4s 8ms/step - loss: 4.5532 - val_loss: 3.3028\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.30275, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9945 - val_loss: 3.1328\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.30275 to 3.13276, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9133 - val_loss: 3.0938\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.13276 to 3.09376, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8696 - val_loss: 3.0821\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.09376 to 3.08211, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8461 - val_loss: 3.1188\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 3.08211\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8125 - val_loss: 3.0815\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.08211 to 3.08154, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7955 - val_loss: 3.0345\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.08154 to 3.03448, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7775 - val_loss: 3.0272\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.03448 to 3.02721, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.7633 - val_loss: 3.0271\n",
            "\n",
            "Epoch 00009: val_loss improved from 3.02721 to 3.02707, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_3.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7509 - val_loss: 3.0659\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3.02707\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7363 - val_loss: 3.0822\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 3.02707\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7504 - val_loss: 3.0570\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.02707\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6957 - val_loss: 3.0334\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.02707\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6825 - val_loss: 3.0784\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 3.02707\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6815 - val_loss: 3.0568\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.02707\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6696 - val_loss: 3.0564\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.02707\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6396 - val_loss: 3.1350\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.02707\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6365 - val_loss: 3.0792\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.02707\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6192 - val_loss: 3.0811\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.02707\n",
            "[Q = 0.4]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 8ms/step - loss: 5.6969 - val_loss: 3.5688\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.56878, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.2636 - val_loss: 3.4339\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.56878 to 3.43389, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.1613 - val_loss: 3.3241\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.43389 to 3.32414, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.0465 - val_loss: 3.3149\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.32414 to 3.31489, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.0142 - val_loss: 3.2316\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.31489 to 3.23156, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9971 - val_loss: 3.2213\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.23156 to 3.22131, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9700 - val_loss: 3.2306\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 3.22131\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9398 - val_loss: 3.2139\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.22131 to 3.21389, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9334 - val_loss: 3.2342\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 3.21389\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9358 - val_loss: 3.2315\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3.21389\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9081 - val_loss: 3.1597\n",
            "\n",
            "Epoch 00011: val_loss improved from 3.21389 to 3.15968, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8904 - val_loss: 3.1777\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.15968\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8901 - val_loss: 3.1744\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.15968\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8861 - val_loss: 3.1993\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 3.15968\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8756 - val_loss: 3.1712\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.15968\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8769 - val_loss: 3.1520\n",
            "\n",
            "Epoch 00016: val_loss improved from 3.15968 to 3.15196, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_4.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8755 - val_loss: 3.1622\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.15196\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8488 - val_loss: 3.1580\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.15196\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8604 - val_loss: 3.1643\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.15196\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8387 - val_loss: 3.1719\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.15196\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8385 - val_loss: 3.2164\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.15196\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8136 - val_loss: 3.1773\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.15196\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.8257 - val_loss: 3.2124\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.15196\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8048 - val_loss: 3.1768\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 3.15196\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7985 - val_loss: 3.2030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 3.15196\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7734 - val_loss: 3.2556\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.15196\n",
            "[Q = 0.5]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 8ms/step - loss: 6.3468 - val_loss: 3.5689\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.56892, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.2092 - val_loss: 3.4178\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.56892 to 3.41785, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 3.0671 - val_loss: 3.2221\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.41785 to 3.22214, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9572 - val_loss: 3.2137\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.22214 to 3.21367, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9101 - val_loss: 3.1760\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.21367 to 3.17598, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8527 - val_loss: 3.1738\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.17598 to 3.17383, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8375 - val_loss: 3.1106\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.17383 to 3.11060, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8149 - val_loss: 3.1206\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 3.11060\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7837 - val_loss: 3.0658\n",
            "\n",
            "Epoch 00009: val_loss improved from 3.11060 to 3.06579, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7841 - val_loss: 3.0848\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3.06579\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7676 - val_loss: 3.0631\n",
            "\n",
            "Epoch 00011: val_loss improved from 3.06579 to 3.06305, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7752 - val_loss: 3.0365\n",
            "\n",
            "Epoch 00012: val_loss improved from 3.06305 to 3.03645, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_5.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7561 - val_loss: 3.0540\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.03645\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7315 - val_loss: 3.0796\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 3.03645\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7278 - val_loss: 3.0515\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.03645\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7195 - val_loss: 3.0656\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.03645\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7159 - val_loss: 3.0493\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.03645\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7018 - val_loss: 3.0779\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.03645\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6928 - val_loss: 3.0461\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.03645\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6718 - val_loss: 3.0779\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.03645\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6593 - val_loss: 3.0812\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.03645\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6781 - val_loss: 3.0836\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.03645\n",
            "[Q = 0.6]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 4s 8ms/step - loss: 7.4249 - val_loss: 3.3115\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.31146, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.9578 - val_loss: 3.0455\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.31146 to 3.04549, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.8138 - val_loss: 2.9845\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.04549 to 2.98453, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6998 - val_loss: 2.9119\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.98453 to 2.91194, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.6215 - val_loss: 2.8329\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.91194 to 2.83295, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5790 - val_loss: 2.7978\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.83295 to 2.79777, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5387 - val_loss: 2.7722\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.79777 to 2.77215, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5192 - val_loss: 2.8174\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.77215\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4987 - val_loss: 2.7652\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.77215 to 2.76519, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4986 - val_loss: 2.7132\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.76519 to 2.71317, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.4751 - val_loss: 2.7106\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.71317 to 2.71064, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 2.4571 - val_loss: 2.7066\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.71064 to 2.70655, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4602 - val_loss: 2.7150\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.70655\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4513 - val_loss: 2.7726\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.70655\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4535 - val_loss: 2.7123\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.70655\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4399 - val_loss: 2.6719\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.70655 to 2.67194, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4356 - val_loss: 2.7227\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.67194\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4157 - val_loss: 2.7040\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.67194\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4170 - val_loss: 2.7254\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.67194\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4165 - val_loss: 2.6624\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.67194 to 2.66241, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_6.h5\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.4031 - val_loss: 2.7090\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.66241\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3825 - val_loss: 2.6677\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.66241\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3859 - val_loss: 2.6716\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 2.66241\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3793 - val_loss: 2.6800\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.66241\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3850 - val_loss: 2.6813\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.66241\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3876 - val_loss: 2.7060\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 2.66241\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3713 - val_loss: 2.6888\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.66241\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3559 - val_loss: 2.6906\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.66241\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3507 - val_loss: 2.6731\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 2.66241\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3471 - val_loss: 2.6655\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.66241\n",
            "[Q = 0.7]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 8ms/step - loss: 8.4938 - val_loss: 2.8177\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.81765, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.5528 - val_loss: 2.6161\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.81765 to 2.61611, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3718 - val_loss: 2.4953\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.61611 to 2.49528, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.2741 - val_loss: 2.3721\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.49528 to 2.37211, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1839 - val_loss: 2.3698\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.37211 to 2.36985, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.1425 - val_loss: 2.3191\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.36985 to 2.31912, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0954 - val_loss: 2.2748\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.31912 to 2.27476, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0579 - val_loss: 2.2655\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.27476 to 2.26549, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0598 - val_loss: 2.2221\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.26549 to 2.22209, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0313 - val_loss: 2.2993\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.22209\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0544 - val_loss: 2.1917\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.22209 to 2.19174, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0199 - val_loss: 2.1900\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.19174 to 2.19000, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9994 - val_loss: 2.2005\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.19000\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9946 - val_loss: 2.2188\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.19000\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9740 - val_loss: 2.1687\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.19000 to 2.16872, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9668 - val_loss: 2.1645\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.16872 to 2.16454, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9733 - val_loss: 2.1419\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.16454 to 2.14191, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9614 - val_loss: 2.1801\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.14191\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9620 - val_loss: 2.1761\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.14191\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9539 - val_loss: 2.1506\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.14191\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9392 - val_loss: 2.1478\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.14191\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9394 - val_loss: 2.1577\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.14191\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9496 - val_loss: 2.1640\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 2.14191\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9204 - val_loss: 2.1431\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.14191\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9432 - val_loss: 2.1330\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.14191 to 2.13301, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9301 - val_loss: 2.2212\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 2.13301\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9397 - val_loss: 2.1373\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.13301\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9283 - val_loss: 2.1479\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.13301\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9373 - val_loss: 2.1313\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.13301 to 2.13134, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9142 - val_loss: 2.1388\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.13134\n",
            "Epoch 31/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 1.9176 - val_loss: 2.1247\n",
            "\n",
            "Epoch 00031: val_loss improved from 2.13134 to 2.12474, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 32/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9058 - val_loss: 2.1356\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 2.12474\n",
            "Epoch 33/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9009 - val_loss: 2.1424\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 2.12474\n",
            "Epoch 34/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8971 - val_loss: 2.1224\n",
            "\n",
            "Epoch 00034: val_loss improved from 2.12474 to 2.12245, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 35/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9072 - val_loss: 2.1338\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.12245\n",
            "Epoch 36/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8982 - val_loss: 2.1386\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 2.12245\n",
            "Epoch 37/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8965 - val_loss: 2.1164\n",
            "\n",
            "Epoch 00037: val_loss improved from 2.12245 to 2.11639, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_7.h5\n",
            "Epoch 38/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8835 - val_loss: 2.1303\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 2.11639\n",
            "Epoch 39/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9017 - val_loss: 2.1648\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 2.11639\n",
            "Epoch 40/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8899 - val_loss: 2.1253\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 2.11639\n",
            "Epoch 41/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8790 - val_loss: 2.1388\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 2.11639\n",
            "Epoch 42/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8746 - val_loss: 2.1425\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 2.11639\n",
            "Epoch 43/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8670 - val_loss: 2.2192\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 2.11639\n",
            "Epoch 44/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8840 - val_loss: 2.1223\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 2.11639\n",
            "Epoch 45/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8702 - val_loss: 2.1403\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 2.11639\n",
            "Epoch 46/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8783 - val_loss: 2.1229\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 2.11639\n",
            "Epoch 47/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8619 - val_loss: 2.1330\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 2.11639\n",
            "[Q = 0.8]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 4s 8ms/step - loss: 9.4225 - val_loss: 3.2934\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.29339, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.3636 - val_loss: 2.0809\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.29339 to 2.08088, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.9026 - val_loss: 1.9979\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.08088 to 1.99789, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.8069 - val_loss: 1.9067\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.99789 to 1.90668, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.7297 - val_loss: 1.8137\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.90668 to 1.81370, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.6726 - val_loss: 1.7524\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.81370 to 1.75242, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5918 - val_loss: 1.7114\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.75242 to 1.71138, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5588 - val_loss: 1.7682\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.71138\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5516 - val_loss: 1.7024\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.71138 to 1.70239, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5271 - val_loss: 1.6657\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.70239 to 1.66567, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.5255 - val_loss: 1.6433\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.66567 to 1.64326, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4867 - val_loss: 1.6269\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.64326 to 1.62695, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4859 - val_loss: 1.6100\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.62695 to 1.61003, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4763 - val_loss: 1.5959\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.61003 to 1.59594, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4525 - val_loss: 1.6744\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.59594\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4501 - val_loss: 1.5894\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.59594 to 1.58937, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4347 - val_loss: 1.5687\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.58937 to 1.56875, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4500 - val_loss: 1.5965\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.56875\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4290 - val_loss: 1.5647\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.56875 to 1.56473, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4270 - val_loss: 1.6109\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.56473\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4233 - val_loss: 1.5758\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.56473\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4137 - val_loss: 1.5935\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.56473\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4160 - val_loss: 1.6104\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.56473\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4032 - val_loss: 1.5306\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.56473 to 1.53063, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4029 - val_loss: 1.5377\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.53063\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3880 - val_loss: 1.6474\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.53063\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3929 - val_loss: 1.5281\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.53063 to 1.52814, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3989 - val_loss: 1.5738\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.52814\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.4009 - val_loss: 1.5290\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.52814\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3831 - val_loss: 1.5223\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.52814 to 1.52234, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 31/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3740 - val_loss: 1.5624\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.52234\n",
            "Epoch 32/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3976 - val_loss: 1.5242\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.52234\n",
            "Epoch 33/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3798 - val_loss: 1.5330\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.52234\n",
            "Epoch 34/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3716 - val_loss: 1.5206\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.52234 to 1.52059, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 35/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3613 - val_loss: 1.5142\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.52059 to 1.51418, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_8.h5\n",
            "Epoch 36/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3707 - val_loss: 1.5428\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.51418\n",
            "Epoch 37/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3729 - val_loss: 1.5638\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.51418\n",
            "Epoch 38/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3598 - val_loss: 1.5908\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.51418\n",
            "Epoch 39/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3582 - val_loss: 1.5153\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.51418\n",
            "Epoch 40/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3554 - val_loss: 1.5269\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.51418\n",
            "Epoch 41/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3542 - val_loss: 1.5265\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.51418\n",
            "Epoch 42/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3403 - val_loss: 1.5216\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.51418\n",
            "Epoch 43/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3440 - val_loss: 1.5161\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.51418\n",
            "Epoch 44/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3478 - val_loss: 1.5178\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.51418\n",
            "Epoch 45/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.3311 - val_loss: 1.5204\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.51418\n",
            "[Q = 0.9]\n",
            "Epoch 1/200\n",
            "287/287 [==============================] - 3s 8ms/step - loss: 8.7775 - val_loss: 3.1617\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.16170, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 2/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.7474 - val_loss: 2.6567\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.16170 to 2.65669, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 3/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 2.0702 - val_loss: 1.2287\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.65669 to 1.22867, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 4/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.1060 - val_loss: 1.1078\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.22867 to 1.10775, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 5/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.0392 - val_loss: 1.0838\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.10775 to 1.08378, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 6/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 1.0126 - val_loss: 1.0419\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.08378 to 1.04194, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 7/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9716 - val_loss: 1.0111\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.04194 to 1.01111, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 8/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9328 - val_loss: 1.0503\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.01111\n",
            "Epoch 9/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.9212 - val_loss: 0.9621\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.01111 to 0.96215, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 10/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8790 - val_loss: 0.9586\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.96215 to 0.95865, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 11/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8580 - val_loss: 0.9493\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.95865 to 0.94929, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 12/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8495 - val_loss: 0.9060\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.94929 to 0.90604, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 13/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8286 - val_loss: 0.9267\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.90604\n",
            "Epoch 14/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8403 - val_loss: 0.8931\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.90604 to 0.89306, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 15/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8239 - val_loss: 0.9118\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.89306\n",
            "Epoch 16/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8169 - val_loss: 0.8768\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.89306 to 0.87677, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 17/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8059 - val_loss: 0.8771\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.87677\n",
            "Epoch 18/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.8123 - val_loss: 0.8930\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.87677\n",
            "Epoch 19/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.8097 - val_loss: 0.8756\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.87677 to 0.87555, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 20/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.7929 - val_loss: 0.8635\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.87555 to 0.86348, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 21/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.7950 - val_loss: 0.8680\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.86348\n",
            "Epoch 22/200\n",
            "287/287 [==============================] - 2s 7ms/step - loss: 0.7948 - val_loss: 0.8996\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.86348\n",
            "Epoch 23/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7856 - val_loss: 0.8668\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.86348\n",
            "Epoch 24/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7848 - val_loss: 0.8587\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.86348 to 0.85870, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 25/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7803 - val_loss: 0.8366\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.85870 to 0.83661, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 26/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7777 - val_loss: 0.8596\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.83661\n",
            "Epoch 27/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7695 - val_loss: 0.8364\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.83661 to 0.83638, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 28/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7715 - val_loss: 0.8549\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.83638\n",
            "Epoch 29/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7665 - val_loss: 0.8838\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.83638\n",
            "Epoch 30/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7728 - val_loss: 0.8442\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.83638\n",
            "Epoch 31/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7729 - val_loss: 0.8531\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.83638\n",
            "Epoch 32/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7645 - val_loss: 0.8482\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.83638\n",
            "Epoch 33/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7674 - val_loss: 0.8700\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.83638\n",
            "Epoch 34/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7789 - val_loss: 0.8323\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.83638 to 0.83230, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 35/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7584 - val_loss: 0.8244\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.83230 to 0.82437, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 36/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7487 - val_loss: 0.8657\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.82437\n",
            "Epoch 37/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7587 - val_loss: 0.8294\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.82437\n",
            "Epoch 38/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7607 - val_loss: 0.8750\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.82437\n",
            "Epoch 39/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7596 - val_loss: 0.8303\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.82437\n",
            "Epoch 40/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7476 - val_loss: 0.8378\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.82437\n",
            "Epoch 41/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7508 - val_loss: 0.8063\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.82437 to 0.80625, saving model to /content/drive/MyDrive/SolarGen/model/m2_best_9.h5\n",
            "Epoch 42/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7380 - val_loss: 0.8310\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.80625\n",
            "Epoch 43/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7407 - val_loss: 0.8084\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.80625\n",
            "Epoch 44/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7373 - val_loss: 0.8325\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.80625\n",
            "Epoch 45/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7441 - val_loss: 0.8290\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.80625\n",
            "Epoch 46/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7403 - val_loss: 0.8331\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.80625\n",
            "Epoch 47/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7417 - val_loss: 0.8224\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.80625\n",
            "Epoch 48/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7362 - val_loss: 0.8189\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.80625\n",
            "Epoch 49/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7326 - val_loss: 0.8225\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.80625\n",
            "Epoch 50/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7339 - val_loss: 0.8385\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.80625\n",
            "Epoch 51/200\n",
            "287/287 [==============================] - 2s 6ms/step - loss: 0.7320 - val_loss: 0.8123\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.80625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ5RdPOmd8dR",
        "outputId": "fe136398-492a-49f1-c3ba-2b18c32c5733"
      },
      "source": [
        "losses = []\r\n",
        "\r\n",
        "for index in range(9):\r\n",
        "    print('[m1_best_%d.h5]'%((index+1)))\r\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m1_best_%d.h5'%((index+1)),\r\n",
        "                                    custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    losses.append(model.evaluate(X_valid1, Y_valid1))\r\n",
        "\r\n",
        "for index in range(9):\r\n",
        "    print('[m2_best_%d.h5]'%((index+1)))\r\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m2_best_%d.h5'%((index+1)),\r\n",
        "                                    custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    losses.append(model.evaluate(X_valid2, Y_valid2))\r\n",
        "\r\n",
        "np.average(losses)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[m1_best_1.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 1.4320\n",
            "[m1_best_2.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.3064\n",
            "[m1_best_3.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.7335\n",
            "[m1_best_4.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.9252\n",
            "[m1_best_5.h5]\n",
            "492/492 [==============================] - 2s 2ms/step - loss: 2.8446\n",
            "[m1_best_6.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.5094\n",
            "[m1_best_7.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.0406\n",
            "[m1_best_8.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 1.4743\n",
            "[m1_best_9.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 0.7739\n",
            "[m2_best_1.h5]\n",
            "492/492 [==============================] - 2s 2ms/step - loss: 1.5246\n",
            "[m2_best_2.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.5308\n",
            "[m2_best_3.h5]\n",
            "492/492 [==============================] - 2s 2ms/step - loss: 3.0271\n",
            "[m2_best_4.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 3.1520\n",
            "[m2_best_5.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 3.0365\n",
            "[m2_best_6.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.6624\n",
            "[m2_best_7.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 2.1164\n",
            "[m2_best_8.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 1.5142\n",
            "[m2_best_9.h5]\n",
            "492/492 [==============================] - 2s 3ms/step - loss: 0.8063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1894445750448437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "zrWUC34yPpqR",
        "outputId": "1e5fc792-dde9-4426-c57f-034f5d64e88d"
      },
      "source": [
        "import matplotlib._color_data as mcd\r\n",
        "\r\n",
        "nth_data = 0\r\n",
        "fig, ax = plt.subplots(figsize=(16,6))\r\n",
        "ax.scatter([i+1 for i in range(48)], X_test[nth_data][:,1], color='k')\r\n",
        "colors = [\"xkcd:azure\", \"xkcd:green\", \"xkcd:chartreuse\", \"xkcd:goldenrod\",\r\n",
        "          \"xkcd:orange\", \"xkcd:coral\", \"xkcd:brown\", \"xkcd:magenta\", \"xkcd:violet\"]\r\n",
        "for index in range(9):\r\n",
        "    model1 = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m1_best_%d.h5'%((index+1)),\r\n",
        "                                       custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    model2 = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m2_best_%d.h5'%((index+1)),\r\n",
        "                                       custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    color =  colors[index]\r\n",
        "    ax.scatter([i+49 for i in range(48)], model1.predict(np.array([X_test[nth_data]])), color=color, label=\"%.1f\"%((index+1)*0.1))\r\n",
        "    ax.scatter([i+97 for i in range(48)], model2.predict(np.array([X_test[nth_data]])), color=color, label=\"%.1f\"%((index+1)*0.1))\r\n",
        "    ax.legend()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88105a92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8806e5e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f881083e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880bbc2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880a9f1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f881071f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880e2f6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880960bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88105262f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88104eda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880c5f7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880c60ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880cfa5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88104cba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f881134b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880d130730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8808b45e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8809fa16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpPgYzQZnULB",
        "outputId": "c40378c3-dfd9-4f19-c3bf-3ff056ec2c68"
      },
      "source": [
        "results1 = pd.DataFrame()\r\n",
        "results2 = pd.DataFrame()\r\n",
        "\r\n",
        "for index in range(9):\r\n",
        "    model1 = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m1_best_%d.h5'%((index+1)),\r\n",
        "                                       custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    model2 = tf.keras.models.load_model('/content/drive/MyDrive/SolarGen/model/m2_best_%d.h5'%((index+1)),\r\n",
        "                                       custom_objects={'pinball_loss': ploss((index+1)*0.1)})\r\n",
        "    pred1 = pd.Series(np.reshape(model1.predict(X_test), -1))\r\n",
        "    pred2 = pd.Series(np.reshape(model2.predict(X_test), -1))\r\n",
        "    results1 = pd.concat([results1, pred1], axis=1)\r\n",
        "    results2 = pd.concat([results2, pred2], axis=1)\r\n",
        "\r\n",
        "results1.columns = quantiles\r\n",
        "results2.columns = quantiles"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88024a6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88073f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8806f29378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880242a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8802584bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8807396488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880740ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8805a228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8810526b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880cf8c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8809ef88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88107518c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88078a69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88104edb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880c5e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880c5f7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8806f298c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f880736f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "2BHhJNaWY18P",
        "outputId": "8efd30ee-5d40-459f-b394-5a88ae941a35"
      },
      "source": [
        "results1 = results1.clip(0.0)\r\n",
        "results2 = results2.clip(0.0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044610</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022958</td>\n",
              "      <td>0.050728</td>\n",
              "      <td>0.195014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007522</td>\n",
              "      <td>0.047817</td>\n",
              "      <td>0.199698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.092913</td>\n",
              "      <td>0.169323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.037687</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.097989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.204658</td>\n",
              "      <td>0.179674</td>\n",
              "      <td>0.404393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.214750</td>\n",
              "      <td>0.121043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175784</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102029</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.150421</td>\n",
              "      <td>0.209586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3883</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.346919</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.253466</td>\n",
              "      <td>0.385180</td>\n",
              "      <td>0.530754</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354946</td>\n",
              "      <td>0.474194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3884</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.242750</td>\n",
              "      <td>0.218010</td>\n",
              "      <td>0.565817</td>\n",
              "      <td>0.043010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384123</td>\n",
              "      <td>0.555972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3885</th>\n",
              "      <td>0.047018</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155587</td>\n",
              "      <td>0.203169</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.523659</td>\n",
              "      <td>0.670158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3886</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074810</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.741735</td>\n",
              "      <td>0.312423</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051357</td>\n",
              "      <td>0.531209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3887</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576509</td>\n",
              "      <td>0.092362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3888 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0.1       0.2       0.3  ...       0.7       0.8       0.9\n",
              "0     0.000000  0.000000  0.000000  ...  0.022958  0.050728  0.195014\n",
              "1     0.000000  0.080216  0.000000  ...  0.000000  0.092913  0.169323\n",
              "2     0.037687  0.000000  0.010742  ...  0.204658  0.179674  0.404393\n",
              "3     0.000000  0.000000  0.043886  ...  0.000000  0.214750  0.121043\n",
              "4     0.000000  0.000000  0.175784  ...  0.067316  0.150421  0.209586\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "3883  0.000000  0.346919  0.000000  ...  0.000000  0.354946  0.474194\n",
              "3884  0.000000  0.000000  0.242750  ...  0.000000  0.384123  0.555972\n",
              "3885  0.047018  0.085840  0.000000  ...  0.000000  0.523659  0.670158\n",
              "3886  0.000000  0.074810  0.000000  ...  0.000000  0.051357  0.531209\n",
              "3887  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.166330\n",
              "\n",
              "[3888 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "KTEw5w_7RGKi",
        "outputId": "dc64c5a2-09a2-44cf-c023-0abe33845af9"
      },
      "source": [
        "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = results1.values\n",
        "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = results2.values\n",
        "submission"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q_0.1</th>\n",
              "      <th>q_0.2</th>\n",
              "      <th>q_0.3</th>\n",
              "      <th>q_0.4</th>\n",
              "      <th>q_0.5</th>\n",
              "      <th>q_0.6</th>\n",
              "      <th>q_0.7</th>\n",
              "      <th>q_0.8</th>\n",
              "      <th>q_0.9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.csv_Day7_0h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016506</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183739</td>\n",
              "      <td>0.035805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202587</td>\n",
              "      <td>0.036859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.csv_Day7_0h30m</td>\n",
              "      <td>0.012137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.csv_Day7_1h00m</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183892</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109436</td>\n",
              "      <td>0.143112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.csv_Day7_1h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.503183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037122</td>\n",
              "      <td>0.618546</td>\n",
              "      <td>0.423160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.csv_Day7_2h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102827</td>\n",
              "      <td>0.159175</td>\n",
              "      <td>0.218788</td>\n",
              "      <td>0.123303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7771</th>\n",
              "      <td>80.csv_Day8_21h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.346919</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.253466</td>\n",
              "      <td>0.385180</td>\n",
              "      <td>0.530754</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354946</td>\n",
              "      <td>0.474194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7772</th>\n",
              "      <td>80.csv_Day8_22h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.24275</td>\n",
              "      <td>0.218010</td>\n",
              "      <td>0.565817</td>\n",
              "      <td>0.043010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384123</td>\n",
              "      <td>0.555972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7773</th>\n",
              "      <td>80.csv_Day8_22h30m</td>\n",
              "      <td>0.047018</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.155587</td>\n",
              "      <td>0.203169</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.523659</td>\n",
              "      <td>0.670158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7774</th>\n",
              "      <td>80.csv_Day8_23h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074810</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.741735</td>\n",
              "      <td>0.312423</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051357</td>\n",
              "      <td>0.531209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7775</th>\n",
              "      <td>80.csv_Day8_23h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576509</td>\n",
              "      <td>0.092362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7776 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id     q_0.1     q_0.2  ...     q_0.7     q_0.8     q_0.9\n",
              "0       0.csv_Day7_0h00m  0.000000  0.016506  ...  0.000000  0.202587  0.036859\n",
              "1       0.csv_Day7_0h30m  0.012137  0.000000  ...  0.111068  0.000000  0.365058\n",
              "2       0.csv_Day7_1h00m  0.000008  0.000000  ...  0.000000  0.109436  0.143112\n",
              "3       0.csv_Day7_1h30m  0.000000  0.000000  ...  0.037122  0.618546  0.423160\n",
              "4       0.csv_Day7_2h00m  0.000000  0.000000  ...  0.159175  0.218788  0.123303\n",
              "...                  ...       ...       ...  ...       ...       ...       ...\n",
              "7771  80.csv_Day8_21h30m  0.000000  0.346919  ...  0.000000  0.354946  0.474194\n",
              "7772  80.csv_Day8_22h00m  0.000000  0.000000  ...  0.000000  0.384123  0.555972\n",
              "7773  80.csv_Day8_22h30m  0.047018  0.085840  ...  0.000000  0.523659  0.670158\n",
              "7774  80.csv_Day8_23h00m  0.000000  0.074810  ...  0.000000  0.051357  0.531209\n",
              "7775  80.csv_Day8_23h30m  0.000000  0.000000  ...  0.000000  0.000000  0.166330\n",
              "\n",
              "[7776 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iFsqwXViRGKj",
        "outputId": "43c4e684-dfb3-43f2-bea2-69036974994b"
      },
      "source": [
        "submission.iloc[:48]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q_0.1</th>\n",
              "      <th>q_0.2</th>\n",
              "      <th>q_0.3</th>\n",
              "      <th>q_0.4</th>\n",
              "      <th>q_0.5</th>\n",
              "      <th>q_0.6</th>\n",
              "      <th>q_0.7</th>\n",
              "      <th>q_0.8</th>\n",
              "      <th>q_0.9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.csv_Day7_0h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183739</td>\n",
              "      <td>0.035805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202587</td>\n",
              "      <td>0.036859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.csv_Day7_0h30m</td>\n",
              "      <td>0.012137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.csv_Day7_1h00m</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183892</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109436</td>\n",
              "      <td>0.143112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.csv_Day7_1h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.503183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037122</td>\n",
              "      <td>0.618546</td>\n",
              "      <td>0.423160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.csv_Day7_2h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102827</td>\n",
              "      <td>0.159175</td>\n",
              "      <td>0.218788</td>\n",
              "      <td>0.123303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.csv_Day7_2h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056921</td>\n",
              "      <td>0.018384</td>\n",
              "      <td>0.396164</td>\n",
              "      <td>0.434031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.csv_Day7_3h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048070</td>\n",
              "      <td>0.205618</td>\n",
              "      <td>0.231483</td>\n",
              "      <td>0.031364</td>\n",
              "      <td>0.364749</td>\n",
              "      <td>0.301555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.csv_Day7_3h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146269</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.226502</td>\n",
              "      <td>0.303554</td>\n",
              "      <td>0.086726</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.csv_Day7_4h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114246</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272941</td>\n",
              "      <td>0.215401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.csv_Day7_4h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119566</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053836</td>\n",
              "      <td>0.405510</td>\n",
              "      <td>0.541446</td>\n",
              "      <td>0.094515</td>\n",
              "      <td>0.662514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.csv_Day7_5h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.244732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.944589</td>\n",
              "      <td>0.240761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.753962</td>\n",
              "      <td>0.695815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.csv_Day7_5h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.459308</td>\n",
              "      <td>0.135976</td>\n",
              "      <td>0.332571</td>\n",
              "      <td>0.134505</td>\n",
              "      <td>0.336524</td>\n",
              "      <td>0.620399</td>\n",
              "      <td>0.252526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.csv_Day7_6h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.543973</td>\n",
              "      <td>0.138823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.csv_Day7_6h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.292825</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119950</td>\n",
              "      <td>0.059368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.csv_Day7_7h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.csv_Day7_7h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.509661</td>\n",
              "      <td>0.133900</td>\n",
              "      <td>0.404839</td>\n",
              "      <td>0.618765</td>\n",
              "      <td>1.278542</td>\n",
              "      <td>1.136748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.csv_Day7_8h00m</td>\n",
              "      <td>1.328477</td>\n",
              "      <td>1.311371</td>\n",
              "      <td>0.284139</td>\n",
              "      <td>2.598078</td>\n",
              "      <td>2.349653</td>\n",
              "      <td>3.225963</td>\n",
              "      <td>3.362239</td>\n",
              "      <td>5.332390</td>\n",
              "      <td>5.982596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.csv_Day7_8h30m</td>\n",
              "      <td>3.011003</td>\n",
              "      <td>3.952279</td>\n",
              "      <td>3.074062</td>\n",
              "      <td>7.037195</td>\n",
              "      <td>6.271083</td>\n",
              "      <td>7.596021</td>\n",
              "      <td>8.803377</td>\n",
              "      <td>11.156019</td>\n",
              "      <td>11.265888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.csv_Day7_9h00m</td>\n",
              "      <td>5.571134</td>\n",
              "      <td>8.197633</td>\n",
              "      <td>8.758022</td>\n",
              "      <td>12.392635</td>\n",
              "      <td>10.755891</td>\n",
              "      <td>13.031207</td>\n",
              "      <td>14.732859</td>\n",
              "      <td>16.896931</td>\n",
              "      <td>18.271431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.csv_Day7_9h30m</td>\n",
              "      <td>8.312806</td>\n",
              "      <td>10.765761</td>\n",
              "      <td>15.422463</td>\n",
              "      <td>18.552183</td>\n",
              "      <td>15.681255</td>\n",
              "      <td>19.702316</td>\n",
              "      <td>21.968229</td>\n",
              "      <td>22.518295</td>\n",
              "      <td>26.098827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.csv_Day7_10h00m</td>\n",
              "      <td>11.244231</td>\n",
              "      <td>16.300846</td>\n",
              "      <td>22.473883</td>\n",
              "      <td>22.483618</td>\n",
              "      <td>22.733774</td>\n",
              "      <td>26.223484</td>\n",
              "      <td>27.283543</td>\n",
              "      <td>28.979170</td>\n",
              "      <td>33.813698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.csv_Day7_10h30m</td>\n",
              "      <td>13.360894</td>\n",
              "      <td>20.093517</td>\n",
              "      <td>28.334581</td>\n",
              "      <td>29.578634</td>\n",
              "      <td>27.929268</td>\n",
              "      <td>32.681129</td>\n",
              "      <td>32.874004</td>\n",
              "      <td>35.083618</td>\n",
              "      <td>40.889652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.csv_Day7_11h00m</td>\n",
              "      <td>15.635690</td>\n",
              "      <td>24.164505</td>\n",
              "      <td>32.855583</td>\n",
              "      <td>32.895359</td>\n",
              "      <td>30.853901</td>\n",
              "      <td>38.553699</td>\n",
              "      <td>37.189919</td>\n",
              "      <td>39.755089</td>\n",
              "      <td>45.771362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.csv_Day7_11h30m</td>\n",
              "      <td>16.843597</td>\n",
              "      <td>26.496302</td>\n",
              "      <td>36.724987</td>\n",
              "      <td>35.888927</td>\n",
              "      <td>32.711803</td>\n",
              "      <td>42.603683</td>\n",
              "      <td>40.105202</td>\n",
              "      <td>42.513180</td>\n",
              "      <td>49.271156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.csv_Day7_12h00m</td>\n",
              "      <td>17.407911</td>\n",
              "      <td>28.867113</td>\n",
              "      <td>37.484650</td>\n",
              "      <td>37.959129</td>\n",
              "      <td>35.881721</td>\n",
              "      <td>45.603127</td>\n",
              "      <td>42.071877</td>\n",
              "      <td>45.146641</td>\n",
              "      <td>51.312878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.csv_Day7_12h30m</td>\n",
              "      <td>16.521246</td>\n",
              "      <td>30.414680</td>\n",
              "      <td>38.731579</td>\n",
              "      <td>38.184387</td>\n",
              "      <td>37.842472</td>\n",
              "      <td>46.188343</td>\n",
              "      <td>42.629993</td>\n",
              "      <td>44.652607</td>\n",
              "      <td>51.541527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.csv_Day7_13h00m</td>\n",
              "      <td>15.221942</td>\n",
              "      <td>30.138041</td>\n",
              "      <td>37.963177</td>\n",
              "      <td>36.873802</td>\n",
              "      <td>34.795204</td>\n",
              "      <td>46.712990</td>\n",
              "      <td>41.875488</td>\n",
              "      <td>44.591049</td>\n",
              "      <td>50.582466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.csv_Day7_13h30m</td>\n",
              "      <td>12.836920</td>\n",
              "      <td>29.130548</td>\n",
              "      <td>38.076328</td>\n",
              "      <td>35.231083</td>\n",
              "      <td>33.572346</td>\n",
              "      <td>45.168076</td>\n",
              "      <td>38.752121</td>\n",
              "      <td>42.311096</td>\n",
              "      <td>48.909687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.csv_Day7_14h00m</td>\n",
              "      <td>10.922243</td>\n",
              "      <td>26.651554</td>\n",
              "      <td>34.462730</td>\n",
              "      <td>32.150433</td>\n",
              "      <td>31.776142</td>\n",
              "      <td>41.628975</td>\n",
              "      <td>36.840118</td>\n",
              "      <td>39.523949</td>\n",
              "      <td>45.625923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.csv_Day7_14h30m</td>\n",
              "      <td>8.943920</td>\n",
              "      <td>23.618029</td>\n",
              "      <td>31.192333</td>\n",
              "      <td>28.160761</td>\n",
              "      <td>28.422127</td>\n",
              "      <td>37.026371</td>\n",
              "      <td>31.389717</td>\n",
              "      <td>34.897270</td>\n",
              "      <td>40.901527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.csv_Day7_15h00m</td>\n",
              "      <td>6.970092</td>\n",
              "      <td>19.859949</td>\n",
              "      <td>28.136887</td>\n",
              "      <td>23.091467</td>\n",
              "      <td>22.672068</td>\n",
              "      <td>32.461205</td>\n",
              "      <td>26.162401</td>\n",
              "      <td>28.835026</td>\n",
              "      <td>34.424957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.csv_Day7_15h30m</td>\n",
              "      <td>4.385017</td>\n",
              "      <td>14.805017</td>\n",
              "      <td>21.635288</td>\n",
              "      <td>18.512344</td>\n",
              "      <td>18.008385</td>\n",
              "      <td>27.002033</td>\n",
              "      <td>19.980320</td>\n",
              "      <td>23.164997</td>\n",
              "      <td>27.623941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.csv_Day7_16h00m</td>\n",
              "      <td>2.229403</td>\n",
              "      <td>10.310103</td>\n",
              "      <td>16.229395</td>\n",
              "      <td>12.907559</td>\n",
              "      <td>12.378326</td>\n",
              "      <td>20.879757</td>\n",
              "      <td>13.046163</td>\n",
              "      <td>15.307256</td>\n",
              "      <td>18.827404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.csv_Day7_16h30m</td>\n",
              "      <td>0.723538</td>\n",
              "      <td>7.731280</td>\n",
              "      <td>10.734511</td>\n",
              "      <td>7.018119</td>\n",
              "      <td>7.838010</td>\n",
              "      <td>14.539573</td>\n",
              "      <td>6.629145</td>\n",
              "      <td>10.318639</td>\n",
              "      <td>11.371654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.csv_Day7_17h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.996810</td>\n",
              "      <td>5.933710</td>\n",
              "      <td>3.431139</td>\n",
              "      <td>4.254802</td>\n",
              "      <td>9.073911</td>\n",
              "      <td>2.232600</td>\n",
              "      <td>6.262612</td>\n",
              "      <td>5.067971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.csv_Day7_17h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.839239</td>\n",
              "      <td>3.186308</td>\n",
              "      <td>0.888092</td>\n",
              "      <td>1.404651</td>\n",
              "      <td>3.969206</td>\n",
              "      <td>0.836338</td>\n",
              "      <td>2.788906</td>\n",
              "      <td>1.887706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.csv_Day7_18h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.745221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.769443</td>\n",
              "      <td>0.099434</td>\n",
              "      <td>1.177131</td>\n",
              "      <td>0.638941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.csv_Day7_18h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184540</td>\n",
              "      <td>0.111546</td>\n",
              "      <td>0.198715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.csv_Day7_19h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.298277</td>\n",
              "      <td>0.207602</td>\n",
              "      <td>0.503779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.csv_Day7_19h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.098765</td>\n",
              "      <td>0.065989</td>\n",
              "      <td>0.226397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.csv_Day7_20h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.678058</td>\n",
              "      <td>0.081516</td>\n",
              "      <td>0.213896</td>\n",
              "      <td>0.261485</td>\n",
              "      <td>0.406875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.csv_Day7_20h30m</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.010037</td>\n",
              "      <td>0.132251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156178</td>\n",
              "      <td>0.071325</td>\n",
              "      <td>0.007047</td>\n",
              "      <td>0.571759</td>\n",
              "      <td>0.315350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.csv_Day7_21h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056245</td>\n",
              "      <td>0.075741</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.198136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168978</td>\n",
              "      <td>0.266450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.csv_Day7_21h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009480</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085061</td>\n",
              "      <td>0.389145</td>\n",
              "      <td>0.206681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.csv_Day7_22h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058974</td>\n",
              "      <td>0.101235</td>\n",
              "      <td>0.141090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080291</td>\n",
              "      <td>0.090352</td>\n",
              "      <td>0.083258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.csv_Day7_22h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.csv_Day7_23h00m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.286339</td>\n",
              "      <td>0.010325</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258875</td>\n",
              "      <td>0.326508</td>\n",
              "      <td>0.272020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.csv_Day7_23h30m</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.253207</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.256160</td>\n",
              "      <td>0.184922</td>\n",
              "      <td>0.156817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id      q_0.1      q_0.2  ...      q_0.7      q_0.8      q_0.9\n",
              "0    0.csv_Day7_0h00m   0.000000   0.016506  ...   0.000000   0.202587   0.036859\n",
              "1    0.csv_Day7_0h30m   0.012137   0.000000  ...   0.111068   0.000000   0.365058\n",
              "2    0.csv_Day7_1h00m   0.000008   0.000000  ...   0.000000   0.109436   0.143112\n",
              "3    0.csv_Day7_1h30m   0.000000   0.000000  ...   0.037122   0.618546   0.423160\n",
              "4    0.csv_Day7_2h00m   0.000000   0.000000  ...   0.159175   0.218788   0.123303\n",
              "5    0.csv_Day7_2h30m   0.000000   0.000000  ...   0.018384   0.396164   0.434031\n",
              "6    0.csv_Day7_3h00m   0.000000   0.000000  ...   0.031364   0.364749   0.301555\n",
              "7    0.csv_Day7_3h30m   0.000000   0.000000  ...   0.303554   0.086726   0.000000\n",
              "8    0.csv_Day7_4h00m   0.000000   0.000000  ...   0.000000   0.272941   0.215401\n",
              "9    0.csv_Day7_4h30m   0.000000   0.000000  ...   0.541446   0.094515   0.662514\n",
              "10   0.csv_Day7_5h00m   0.000000   0.000000  ...   0.000000   0.753962   0.695815\n",
              "11   0.csv_Day7_5h30m   0.000000   0.000000  ...   0.336524   0.620399   0.252526\n",
              "12   0.csv_Day7_6h00m   0.000000   0.000000  ...   0.000000   0.000000   0.000000\n",
              "13   0.csv_Day7_6h30m   0.000000   0.000000  ...   0.000000   0.119950   0.059368\n",
              "14   0.csv_Day7_7h00m   0.000000   0.000000  ...   0.077029   0.000000   0.000000\n",
              "15   0.csv_Day7_7h30m   0.000000   0.000000  ...   0.618765   1.278542   1.136748\n",
              "16   0.csv_Day7_8h00m   1.328477   1.311371  ...   3.362239   5.332390   5.982596\n",
              "17   0.csv_Day7_8h30m   3.011003   3.952279  ...   8.803377  11.156019  11.265888\n",
              "18   0.csv_Day7_9h00m   5.571134   8.197633  ...  14.732859  16.896931  18.271431\n",
              "19   0.csv_Day7_9h30m   8.312806  10.765761  ...  21.968229  22.518295  26.098827\n",
              "20  0.csv_Day7_10h00m  11.244231  16.300846  ...  27.283543  28.979170  33.813698\n",
              "21  0.csv_Day7_10h30m  13.360894  20.093517  ...  32.874004  35.083618  40.889652\n",
              "22  0.csv_Day7_11h00m  15.635690  24.164505  ...  37.189919  39.755089  45.771362\n",
              "23  0.csv_Day7_11h30m  16.843597  26.496302  ...  40.105202  42.513180  49.271156\n",
              "24  0.csv_Day7_12h00m  17.407911  28.867113  ...  42.071877  45.146641  51.312878\n",
              "25  0.csv_Day7_12h30m  16.521246  30.414680  ...  42.629993  44.652607  51.541527\n",
              "26  0.csv_Day7_13h00m  15.221942  30.138041  ...  41.875488  44.591049  50.582466\n",
              "27  0.csv_Day7_13h30m  12.836920  29.130548  ...  38.752121  42.311096  48.909687\n",
              "28  0.csv_Day7_14h00m  10.922243  26.651554  ...  36.840118  39.523949  45.625923\n",
              "29  0.csv_Day7_14h30m   8.943920  23.618029  ...  31.389717  34.897270  40.901527\n",
              "30  0.csv_Day7_15h00m   6.970092  19.859949  ...  26.162401  28.835026  34.424957\n",
              "31  0.csv_Day7_15h30m   4.385017  14.805017  ...  19.980320  23.164997  27.623941\n",
              "32  0.csv_Day7_16h00m   2.229403  10.310103  ...  13.046163  15.307256  18.827404\n",
              "33  0.csv_Day7_16h30m   0.723538   7.731280  ...   6.629145  10.318639  11.371654\n",
              "34  0.csv_Day7_17h00m   0.000000   3.996810  ...   2.232600   6.262612   5.067971\n",
              "35  0.csv_Day7_17h30m   0.000000   1.839239  ...   0.836338   2.788906   1.887706\n",
              "36  0.csv_Day7_18h00m   0.000000   0.000000  ...   0.099434   1.177131   0.638941\n",
              "37  0.csv_Day7_18h30m   0.000000   0.000000  ...   0.184540   0.111546   0.198715\n",
              "38  0.csv_Day7_19h00m   0.000000   0.000000  ...   0.298277   0.207602   0.503779\n",
              "39  0.csv_Day7_19h30m   0.000000   0.000000  ...   0.098765   0.065989   0.226397\n",
              "40  0.csv_Day7_20h00m   0.000000   0.000000  ...   0.213896   0.261485   0.406875\n",
              "41  0.csv_Day7_20h30m   0.000365   0.010037  ...   0.007047   0.571759   0.315350\n",
              "42  0.csv_Day7_21h00m   0.000000   0.056245  ...   0.000000   0.168978   0.266450\n",
              "43  0.csv_Day7_21h30m   0.000000   0.000000  ...   0.085061   0.389145   0.206681\n",
              "44  0.csv_Day7_22h00m   0.000000   0.000000  ...   0.080291   0.090352   0.083258\n",
              "45  0.csv_Day7_22h30m   0.000000   0.000000  ...   0.081776   0.000000   0.117803\n",
              "46  0.csv_Day7_23h00m   0.000000   0.000000  ...   0.258875   0.326508   0.272020\n",
              "47  0.csv_Day7_23h30m   0.000000   0.000000  ...   0.256160   0.184922   0.156817\n",
              "\n",
              "[48 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B2b7MSQRGKj"
      },
      "source": [
        "# submission.to_csv(data_path + './data/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}